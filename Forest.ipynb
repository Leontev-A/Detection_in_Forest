{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "106ad71d",
   "metadata": {},
   "source": [
    "#### 1. Загружаем необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb25e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from skimage.io import imread, imshow, imsave\n",
    "from keras.utils.all_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import adam_v2\n",
    "from keras.metrics import Recall\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.applications import EfficientNetB7\n",
    "import albumentations as A\n",
    "from albumentations import(\n",
    "    RandomCrop, ShiftScaleRotate, GaussNoise, MedianBlur,\n",
    "    HorizontalFlip, VerticalFlip, OneOf, Compose, PadIfNeeded,\n",
    "    ElasticTransform, RandomSizedBBoxSafeCrop)\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nums_from_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7bbe5",
   "metadata": {},
   "source": [
    "#### 2. Подготавливаем исходные данные для дальнейшей детекции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1848ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем отдельно те изображения тренировочного датасета, для которых есть разметка (где есть люди).\n",
    "# Разметку сохраняем в YOLO формат для последующей детекции в YOLO v5\n",
    "img_dir = 'C:/ML/Forest/train_dataset_train/train'\n",
    "labels_out_dir = 'C:/ML/Forest/Augmentations/ini/labels'\n",
    "images_out_dir = 'C:/ML/Forest/Augmentations/ini/images'\n",
    "\n",
    "data_df = pd.read_csv(\"C:/ML/Forest/train_dataset_train/train.csv\")\n",
    "for i in range(len(data_df)):\n",
    "    line = data_df.iloc[i]\n",
    "    if line[1] == 0:\n",
    "        continue\n",
    "    else:\n",
    "        img = imread(img_dir + '/' + str(line[0]))\n",
    "        name = str(line[0]).split('.')[0]\n",
    "        s = img.shape\n",
    "        coords = np.array(nums_from_string.get_nums(line[2]))\n",
    "        coords = coords.reshape((int(coords.shape[0]/3),3))\n",
    "        imsave(images_out_dir + '/' + line[0], img)\n",
    "        f = open(labels_out_dir + '/' + name + '.txt', 'w')\n",
    "        for j in range(int(line[1])):\n",
    "            coord = coords[j]\n",
    "            x = coord[0]\n",
    "            y = coord[1]\n",
    "            w = 2 * coord[2]\n",
    "            h = 2 * coord[2]\n",
    "            out_line = '0 ' + str(x/s[1]) + ' ' + str(y/s[0]) + ' ' + str(w/s[1]) + ' ' + str(h/s[0])\n",
    "            f.write(out_line + '\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9991fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Аугментируем и сохраняем аугментированные изображения и файлы разметки для дальнейшей детекции в YOLO v5\n",
    "im_dir = 'C:/ML/Forest/Augmentations/ini/images'\n",
    "lbl_dir = 'C:/ML/Forest/Augmentations/ini/labels'\n",
    "\n",
    "im_out = 'C:/ML/Forest/Augmentations/dataset/images'\n",
    "lbl_out = 'C:/ML/Forest/Augmentations/dataset/labels'\n",
    "\n",
    "aug_num = 28\n",
    "\n",
    "for path, directories, files in os.walk(im_dir):\n",
    "    for file in files:\n",
    "        filename = file.split(\".\")[0]\n",
    "        filepath = path + '/' + file\n",
    "        img = imread(filepath)\n",
    "        f = open(lbl_dir + '/' + filename + '.txt', 'r')\n",
    "        bboxes = []\n",
    "        for line in f:\n",
    "            coords = nums_from_string.get_nums(line[1:-1])\n",
    "            bboxes.append([coords[0], coords[1], coords[2], coords[3], '0'])\n",
    "        f.close()\n",
    "        transform = Compose([\n",
    "                            OneOf([\n",
    "                                HorizontalFlip(p=1),\n",
    "                                VerticalFlip(p=1),\n",
    "                            ], p=0.8),\n",
    "                            OneOf([\n",
    "                                ShiftScaleRotate(shift_limit=[-0.2, 0.2], rotate_limit=0, scale_limit=0, p=1, border_mode=cv2.BORDER_REPLICATE),\n",
    "                                ShiftScaleRotate(shift_limit=0, rotate_limit=[-30, 30], scale_limit=0, p=1, border_mode=cv2.BORDER_REPLICATE),\n",
    "                                ShiftScaleRotate(shift_limit=0, rotate_limit=0, scale_limit=[0, 0.3], p=1, border_mode=cv2.BORDER_REPLICATE),\n",
    "                            ], p=1),\n",
    "                            ], bbox_params=A.BboxParams(format='yolo', min_area=5000), p=1)\n",
    "        for i in range(aug_num):\n",
    "            try:\n",
    "                transformed = transform(image=img, bboxes=bboxes)\n",
    "                transformed_image = transformed['image']\n",
    "                transformed_bboxes = transformed['bboxes']\n",
    "                if transformed_bboxes != []:\n",
    "                    imsave(im_out + '/' + filename + '_' + str(i + 1) + '.png', transformed_image)\n",
    "                    f = open(lbl_out + '/' + filename + '_' + str(i + 1) + '.txt', 'w')\n",
    "                    for j in transformed_bboxes:\n",
    "                        out_line = '0 %f %f %f %f' % (j[0], j[1], j[2], j[3])\n",
    "                        f.write(out_line + '\\n')\n",
    "                    f.close()\n",
    "            except:\n",
    "                print('wrong bbox')\n",
    "#        break\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a3d8a3",
   "metadata": {},
   "source": [
    "#### 3. Перед детекцией отфильтруем изображения тестового датасета по наличию на них людей с помощью бинарной классификации. Для этого напишем и обучим модель классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8faa598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем изображения тренировочного датасета и их разметки\n",
    "img_dir = 'C:/ML/Forest/train_dataset_train/train'\n",
    "size = 512\n",
    "num_classes = 2\n",
    "\n",
    "data_df = pd.read_csv(\"C:/ML/Forest/train_dataset_train/train.csv\")\n",
    "detected_csv = pd.DataFrame(columns=['ID_img', 'count_region', 'region_shape'])\n",
    "X = []\n",
    "Y = []\n",
    "for_weight = []\n",
    "for i in range(len(data_df)):\n",
    "    line = data_df.iloc[i]\n",
    "    img = imread(img_dir + '/' + str(line[0]))\n",
    "    img = cv2.resize(img, dsize = [size, size], interpolation=cv2.INTER_NEAREST)\n",
    "    X.append(img)\n",
    "    lb = line[1]\n",
    "    if lb > 0:\n",
    "        lb = 1\n",
    "    for_weight.append(lb)\n",
    "    cat_lb = to_categorical(lb, num_classes=num_classes+1, dtype=int)\n",
    "    cat_lb = np.delete(cat_lb, 2)\n",
    "    Y.append(cat_lb)\n",
    "#    break\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "for_weight = np.array(for_weight).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00eb1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Рассчитаем весовые коэффициенты обоих классов для балансировки\n",
    "weight_array = compute_class_weight(class_weight='balanced', classes=np.unique(for_weight), y=for_weight)\n",
    "\n",
    "weight_dict = dict(zip(np.unique(for_weight), weight_array))\n",
    "weight_dict = dict(sorted(weight_dict.items()))\n",
    "new_dict = {}\n",
    "for i in range(len(weight_dict)):\n",
    "    new_dict[i] = weight_dict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7aa5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим рассчитанные весовые коэффициенты\n",
    "new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d505167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим размерность тензора с изображениями\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa50a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим размерность тензора с разметкой\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f700fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим тренировочный датасет на тренировочную и тестовую выборки\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалим уже неиспользуемые переменные чтобы отчистить оперативную память\n",
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6947a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем предобученную модель классификации\n",
    "base_model = EfficientNetB7(weights='imagenet', input_shape=(size,size,3), include_top=False)\n",
    "base_out = base_model.output\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a49e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим архитектуру предобученной модели\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febec573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Допишем голову классификатора, которая будет обучаться на наших выборках\n",
    "x = base_out\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation = 'relu')(x)\n",
    "x = Dense(128, activation = 'relu')(x)\n",
    "outputs = Dense(num_classes, activation = 'softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b04d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем модель и проверяем архитектуру\n",
    "model = Model(inputs=base_model.inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Компилируем и обучаем модель\n",
    "model.compile(optimizer=adam_v2.Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=[\"accuracy\", Recall()])\n",
    "model.fit(X_train, Y_train, epochs=5, validation_data=(X_test, Y_test), class_weight=new_dict, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4760437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказываем наличие/отсутствие людей на изображениях тестового датасета и сохраняем только те, где есть люди\n",
    "# для дальнейшей детекции\n",
    "im_dir = 'C:/ML/Forest/test_dataset_test/test'\n",
    "detected_out_path = 'C:/ML/Forest/detected'\n",
    "\n",
    "for path, directories, files in os.walk(im_dir):\n",
    "    for file in files:\n",
    "        filename = file.split(\".\")[0]\n",
    "        filepath = path + '/' + file\n",
    "        image = imread(filepath)\n",
    "        img = cv2.resize(image, dsize = [size, size], interpolation=cv2.INTER_NEAREST)\n",
    "        test_img_input = np.expand_dims(img, 0)\n",
    "        prediction = np.squeeze(model.predict(test_img_input))\n",
    "        prediction = np.argmax(prediction)\n",
    "        if prediction == 1:\n",
    "            imsave(detected_out_path + '/' + file, image)\n",
    "#        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ace1c7",
   "metadata": {},
   "source": [
    "#### 4. Обучим модель детекции YOLO v5 на ранее аугментированном тренировочном датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9233c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переходим в рабочую папку модели YOLO v5\n",
    "%cd C:/ML/Object_detection/Yolo_5/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем предобученную модель YOLO v5\n",
    "!python train.py --img 1280 --batch 2 --epochs 50 --data forest_aug.yaml --weights yolov5m.pt --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb71da05",
   "metadata": {},
   "source": [
    "#### 5. Предскажем результат на ранее отфильтрованной классификатором тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becfee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем результаты обучения на лучшей эпохе\n",
    "model = torch.hub.load('C:/ML/Object_detection/Yolo_5/yolov5', 'custom', 'C:/ML/Object_detection/Yolo_5/yolov5/runs/train/exp8/weights/best.pt', source='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5539609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказываем результаты детекции только для тех изображений теста, в которых были найдены люди\n",
    "# Сохраняем полученные результаты в csv формат, в соответствии с требованиями задачи\n",
    "test_img_dir = 'C:/ML/Forest/test_dataset_test/test'\n",
    "df = pd.DataFrame(columns=['ID_img', 'region_shape'])\n",
    "out_path = 'C:/ML/Forest/Results/Attempt_6/'\n",
    "\n",
    "for path, directories, files in os.walk('C:/ML/Forest/detected'):\n",
    "    break\n",
    "\n",
    "data_df_test = pd.read_csv(\"C:/ML/Forest/omsk/sample_solution.csv\")\n",
    "for i in range(len(data_df_test)):\n",
    "    line = data_df_test.iloc[i]\n",
    "    img = imread(test_img_dir + '/' + line[0])\n",
    "    coord = '['\n",
    "    results = model(img)\n",
    "    res = results.xywh\n",
    "    res_r = res[0].detach().numpy()\n",
    "    res = np.around(res_r)\n",
    "    res = res.astype('int')\n",
    "    for_sort = []\n",
    "    for i in range(res.shape[0]):\n",
    "        x = res[i][0]\n",
    "        y = res[i][1]\n",
    "        if res[i][2] <= res[i][3]:\n",
    "            r = int(round(res[i][2] / 2))\n",
    "        else:\n",
    "            r = int(round(res[i][3] / 2))\n",
    "        for_sort.append([x, y, r])\n",
    "    for_sort = sorted(for_sort)\n",
    "    for indx in range(res.shape[0]):\n",
    "        x = for_sort[indx][0]\n",
    "        y = for_sort[indx][1]\n",
    "        r = for_sort[indx][2]\n",
    "        coord = coord + '\\'{\"cx\":%d,\"cy\":%d,\"r\":%d}\\'' % (x, y, r) + ','\n",
    "    l = len(coord)\n",
    "    coord = coord[:l-1] + ']'\n",
    "    if coord == ']':\n",
    "        coord = 0\n",
    "    if line[0] not in files:\n",
    "        coord = 0\n",
    "    new_line = {'ID_img'      : line[0],\n",
    "                'region_shape': coord}\n",
    "    df = df.append(new_line, ignore_index=True)\n",
    "#    break\n",
    "df.to_csv(out_path + 'Forest_6.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
